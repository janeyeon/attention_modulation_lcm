{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad88ce2a-b919-403b-82c1-b84b034e043a",
   "metadata": {},
   "source": [
    "Version of diffusers: 0.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f3df295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "# from config_controlnet_boxdiff import RunConfig\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from controlnet_aux import OpenposeDetector\n",
    "from controlnet_aux.open_pose import draw_poses\n",
    "\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import PIL.Image\n",
    "# from controlnet_aux.open_pose import PoseResult, BodyResult, Keypoint\n",
    "# from controlnet_aux.util import HWC3\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "from drawer import draw_rectangle, DashedImageDraw\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from _config import *\n",
    "from _utils import *\n",
    "from _group_dict import *\n",
    "from _load_model import *\n",
    "from _generate_map import *\n",
    "from _preprocess_patch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c335384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##########################  Choose one image and get keylayout information ##############################\n",
    "\n",
    "config = RunConfig()\n",
    "group_dictionary, keylayout_data = makeGroupdict(config)\n",
    "timesteps, sp_sz, bsz, mod_forward_orig, pipe = loadControlNet(config)\n",
    "\n",
    "# Timesteps: full timesteps -> 999 to 0\n",
    "# sp_sz: latent_size\n",
    "# bsz: batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2bf7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_prompt, global_H, global_W, global_canvas_H, global_canvas_W = getGlobalInfo(keylayout_data, config)\n",
    "# group_ids = getGroupIds(group_dictionary, config)\n",
    "\n",
    "# global_prompt: global prompt -> ex)'a group of children and a dog posing for a picture.'\n",
    "# global_H, global_W: Size of image about to generate\n",
    "# global_canvas_H, global_canvas_W: Size of canva for generation => should be divided by 64\n",
    "# group_ids: id that is consists of group ex) ['0', '2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153d0004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pose_map,\\\n",
    "# group_L_maps,\\\n",
    "# inst_obj_L_maps,\\\n",
    "# inst_obj_boxes, \\\n",
    "# inst_obj_L_maps_small,\\\n",
    "# group_maps,\\\n",
    "# inst_maps,\\\n",
    "# obj_maps,\\\n",
    "# group_prompt_dic,\\\n",
    "# inst_obj_prompt_dic,\\\n",
    "# poses, \\\n",
    "# pose_masks, \\\n",
    "# nose2neck_lengths,\\\n",
    "# group_boxes,\\\n",
    "# inst_boxes,\\\n",
    "# obj_boxes, \\\n",
    "# inst_obj_maps,\\\n",
    "# pose_box_map = generate_map( \n",
    "#               global_canvas_H=global_canvas_H, \n",
    "#               global_canvas_W=global_canvas_W, \n",
    "#               config=config, \n",
    "#               global_prompt=global_prompt,\n",
    "#               group_ids=group_ids,\n",
    "#               group_dictionary=group_dictionary\n",
    "#               )\n",
    "\n",
    "# pose_map: human pose map - 졸라맨처럼 생긴 것 -> np.ndarray\n",
    "\n",
    "# pose_box_map: pose_map + bbox -> PIL.Image file\n",
    "\n",
    "# inst_obj_maps: 각각의 intance(object+person)들의 mask들 - overlap 되어있지 않게 구성되어있음 [intance_num, global_canvas_H, global_canvas_W, 2]\n",
    "#                      마지막 2 dim은 0-dim -> occupy or not / 1-dim -> instance/object key\n",
    "\n",
    "# inst_obj_L_maps_small: #? bbox area인데 object+instance-1만큼의 작은 map임. [object+instance-1, 72, 96, 2] 마지막 dim은 다른것과 동일\n",
    "\n",
    "# group_L_maps: group_maps와 유사한데 크기만 작음\n",
    "\n",
    "# inst_obj_L_maps: inst_obj_maps와 유사한데 크기만 작음\n",
    "\n",
    "# inst_obj_boxes: intance와 object의 bbox들 의 좌표들 -> List[tuple(4)]\n",
    "\n",
    "# obj_boxes: object들의 bbox 좌표들 -> List[tuple(4)]\n",
    "\n",
    "# inst_boxes: instance bbox 좌표들 -> List[tuple(4)]\n",
    "\n",
    "# group_boxes: group bbox 좌표들 -> List[tuple(4)]\n",
    "\n",
    "# nose2neck_lengths: 각 person들의 noise2neck lengths -> sorting 되어있음\n",
    "\n",
    "# pose_masks: 각 person들의 mask들 - overlap 되어있음. pose map thicken 한듯 [person_num, global_canvas_H, global_canvas_W]\n",
    "\n",
    "# poses: 각 person들의 pose 정보(keypoints등)이 담겨 있는 List[person_num]\n",
    "\n",
    "# inst_obj_prompt_dic: instance(person)과 object에 대한 prompt들이 담겨있는 dictionary.\n",
    "#                      instance는 key가 1에서부터 시작하고(0엔 아무것도 안들어있음)\n",
    "#                      object는 key가 1001에서부터 시작\n",
    "\n",
    "# group_prompt_dic: 각각의 group들에 대한 prompt - 1부터 시작\n",
    "\n",
    "# obj_maps: 각각의 object에 대한 영역 (bbox 안)이 mask로 되어있음 [object_num, global_canvas_H, global_canvas_W, 2]\n",
    "#                      마지막 2 dim은 0-dim -> occupy or not / 1-dim -> object key\n",
    "\n",
    "# inst_maps: 각각의 instance(person)에 대한 영역 (bbox 안)이 mask로 되어있음 [person_num, global_canvas_H, global_canvas_W, 2]\n",
    "#                      마지막 2 dim은 0-dim -> occupy or not / 1-dim -> instance key\n",
    "\n",
    "# group_maps:각각의 group에 대한 영역 (bbox 안)이 mask로 되어있음 [group_num, 1,  global_canvas_H, global_canvas_W]\n",
    "#                      group_num dim은 group_idx (group_key)가 담겨있음 ex) group_idx가 2인 group은 해당 bbox가 2로 채워져있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a7df4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from diffusers.loaders import FromSingleFileMixin, LoraLoaderMixin, TextualInversionLoaderMixin\n",
    "from diffusers.models import AutoencoderKL, ControlNetModel, UNet2DConditionModel\n",
    "from diffusers.schedulers import KarrasDiffusionSchedulers\n",
    "from diffusers.utils import (\n",
    "    is_accelerate_available,\n",
    "    is_accelerate_version,\n",
    "    is_compiled_module,\n",
    "    logging,\n",
    "    randn_tensor,\n",
    "    replace_example_docstring,\n",
    ")\n",
    "from diffusers.pipelines.pipeline_utils import DiffusionPipeline\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput\n",
    "from diffusers.pipelines.stable_diffusion.safety_checker import StableDiffusionSafetyChecker\n",
    "# from .multicontrolnet import MultiControlNetModel\n",
    "import copy\n",
    "from diffusers.models.attention_processor import (\n",
    "    AttnProcessor2_0,\n",
    "    LoRAAttnProcessor2_0,\n",
    "    LoRAXFormersAttnProcessor,\n",
    "    XFormersAttnProcessor,\n",
    ")\n",
    "\n",
    "### DenseDiff Global Param\n",
    "reg_part = 0.7\n",
    "sreg = .3\n",
    "creg = 1.\n",
    "COUNT = 0\n",
    "DENSEDIFF_ON = True\n",
    "NUM_INFERENCE_STEPS = config.num_inference_steps\n",
    "\n",
    "### padding\n",
    "use_padded_latents = True\n",
    "\n",
    "\n",
    "### Hierachical Refinement\n",
    " \n",
    "def upcast_vae(pipe):\n",
    "    dtype = pipe.vae.dtype\n",
    "    pipe.vae.to(dtype=torch.float32)\n",
    "    use_torch_2_0_or_xformers = isinstance(\n",
    "        pipe.vae.decoder.mid_block.attentions[0].processor,\n",
    "        (\n",
    "            AttnProcessor2_0,\n",
    "            XFormersAttnProcessor,\n",
    "            LoRAXFormersAttnProcessor,\n",
    "            LoRAAttnProcessor2_0,\n",
    "        ),\n",
    "    )\n",
    "    # if xformers or torch_2_0 is used attention block does not need\n",
    "    # to be in float32 which can save lots of memory\n",
    "    if use_torch_2_0_or_xformers:\n",
    "        pipe.vae.post_quant_conv.to(dtype)\n",
    "        pipe.vae.decoder.conv_in.to(dtype)\n",
    "        pipe.vae.decoder.mid_block.to(dtype)\n",
    "    return pipe\n",
    "    \n",
    "# def get_timesteps(pipe, num_inference_steps, strength, device, reverse_ratio):\n",
    "#     # get the original timestep using init_timestep\n",
    "#     init_timestep = min(int(num_inference_steps * strength), num_inference_steps)\n",
    "\n",
    "#     t_start = max(num_inference_steps - init_timestep, 0)\n",
    "#     timesteps = pipe.scheduler.timesteps[t_start * pipe.scheduler.order :]\n",
    "\n",
    "#     # return timesteps, num_inference_steps - t_start\n",
    "#     return timesteps\n",
    "\n",
    "# def double_image(low_res:Image, probability=0.9, enlarge_ratio=2):\n",
    "#     low_res = np.array(low_res)\n",
    "#     image_H, image_W, _ = low_res.shape\n",
    "#     image = cv2.resize(low_res, (image_W * config.enlarge_ratio, image_H * config.enlarge_ratio), cv2.INTER_LANCZOS4)\n",
    "#     image = interpolation_shuffle(image, probability, enlarge_ratio)\n",
    "#     image = np.array(image) / 255.\n",
    "#     image = torch.tensor(image, device='cuda', dtype=torch.float32).unsqueeze(0).permute(0, 3, 1, 2)\n",
    "#     return image\n",
    "\n",
    "def get_timesteps(pipe, num_inference_steps, strength, device, reverse_ratio):\n",
    "    #reverse_ratio: 1: original image, 0: noise image\n",
    "    # get the original timestep using init_timestep\n",
    "    init_timestep = min(int(num_inference_steps * strength), num_inference_steps)\n",
    "\n",
    "    t_start = max(num_inference_steps - init_timestep, 0)\n",
    "    timesteps = pipe.scheduler.timesteps[t_start * pipe.scheduler.order :]\n",
    "\n",
    "    # t_mix = int(1000 * (1 - reverse_ratio))\n",
    "    # mix_steps = int(t_mix/1000 * 50)\n",
    "\n",
    "    # pipe.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "    # timesteps = pipe.scheduler.timesteps\n",
    "    # timesteps_list = list(range(t_mix, 0, -20))\n",
    "    # timesteps = torch.tensor(timesteps_list, dtype=torch.int, device='cuda')\n",
    "    return timesteps\n",
    "\n",
    "\n",
    "def double_image(input_image, enlarge_ratio, probability=0.9):\n",
    "    input_image = np.array(input_image)\n",
    "    # input_image = input_image.transpose((1, 0, 2))\n",
    "    distance=1\n",
    "    # Get the dimensions of the input image\n",
    "    height, width, channels = input_image.shape\n",
    "\n",
    "    # Calculate the new dimensions\n",
    "    new_height = height * enlarge_ratio\n",
    "    new_width = width * enlarge_ratio\n",
    "\n",
    "    # Create an empty expanded image\n",
    "    expanded_image = cv2.resize(input_image, (new_width, new_height), cv2.INTER_LANCZOS4)\n",
    "\n",
    "    # Loop through each pixel in the expanded image\n",
    "    for y in range(new_height):\n",
    "        for x in range(new_width):\n",
    "            if random.random() > probability:\n",
    "                # Find a random pixel within the specified distance in the original image\n",
    "                source_x = min(max(0, int(x / enlarge_ratio) + random.randint(-distance, distance)), width - distance - 1)\n",
    "                source_y = min(max(0, int(y / enlarge_ratio) + random.randint(-distance, distance)), height - distance - 1)\n",
    "                # Copy the pixel from the original image to the expanded image\n",
    "                expanded_image[y, x] = input_image[source_y, source_x]\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    expanded_image = np.array(expanded_image) / 255.\n",
    "    expanded_image = torch.tensor(expanded_image, device='cuda', dtype=torch.float32).unsqueeze(0).permute(0, 3, 1, 2)\n",
    "    return expanded_image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepare_latents(pipe, image, timestep, batch_size, num_images_per_prompt, dtype, device, generator=None):\n",
    "    \"\"\"\n",
    "    if image 가 latent이면 -> 그대로 냅둠\n",
    "    image가 image 면 -> pipe.vae.encoder 통과\n",
    "\n",
    "    이후 scheduler에 맞게 noise를  추가하여 init_latents생성후 return\n",
    "    \"\"\"\n",
    "    if not isinstance(image, (torch.Tensor, PIL.Image.Image, list)):\n",
    "        raise ValueError(\n",
    "            f\"`image` has to be of type `torch.Tensor`, `PIL.Image.Image` or list but is {type(image)}\"\n",
    "        )\n",
    "\n",
    "    image = image.to(device=device, dtype=dtype)\n",
    "\n",
    "    batch_size = batch_size * num_images_per_prompt\n",
    "\n",
    "    if image.shape[1] == 4:\n",
    "        init_latents = image\n",
    "\n",
    "    else:\n",
    "        if isinstance(generator, list) and len(generator) != batch_size:\n",
    "            raise ValueError(\n",
    "                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n",
    "                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n",
    "            )\n",
    "\n",
    "        elif isinstance(generator, list):\n",
    "            init_latents = [\n",
    "                pipe.vae.encode(image[i : i + 1]).latent_dist.sample(generator[i]) for i in range(batch_size)\n",
    "            ]\n",
    "            init_latents = torch.cat(init_latents, dim=0)\n",
    "        else:\n",
    "            init_latents = pipe.vae.encode(image).latent_dist.sample(generator)\n",
    "\n",
    "        init_latents = pipe.vae.config.scaling_factor * init_latents\n",
    "\n",
    "    if batch_size > init_latents.shape[0] and batch_size % init_latents.shape[0] == 0:\n",
    "        # expand init_latents for batch_size\n",
    "        deprecation_message = (\n",
    "            f\"You have passed {batch_size} text prompts (`prompt`), but only {init_latents.shape[0]} initial\"\n",
    "            \" images (`image`). Initial images are now duplicating to match the number of text prompts. Note\"\n",
    "            \" that this behavior is deprecated and will be removed in a version 1.0.0. Please make sure to update\"\n",
    "            \" your script to pass as many initial images as text prompts to suppress this warning.\"\n",
    "        )\n",
    "        deprecate(\"len(prompt) != len(image)\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
    "        additional_image_per_prompt = batch_size // init_latents.shape[0]\n",
    "        init_latents = torch.cat([init_latents] * additional_image_per_prompt, dim=0)\n",
    "    elif batch_size > init_latents.shape[0] and batch_size % init_latents.shape[0] != 0:\n",
    "        raise ValueError(\n",
    "            f\"Cannot duplicate `image` of batch size {init_latents.shape[0]} to {batch_size} text prompts.\"\n",
    "        )\n",
    "    else:\n",
    "        init_latents = torch.cat([init_latents], dim=0)\n",
    "\n",
    "    shape = init_latents.shape\n",
    "    noise = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n",
    "\n",
    "    # get latents\n",
    "    init_latents = pipe.scheduler.add_noise(init_latents, noise, timestep)\n",
    "    latents = init_latents\n",
    "\n",
    "    return latents\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e0e6ae-f663-4027-86c8-35969f37f252",
   "metadata": {},
   "source": [
    "### Apply DenseDiff Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98475e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##################################################################################################\n",
    "def mod_forward(self, hidden_states, encoder_hidden_states=None, attention_mask=None, temb=None, config=None):\n",
    "    residual = hidden_states\n",
    "    if self.spatial_norm is not None:\n",
    "            hidden_states = attn.spatial_norm(hidden_states, temb)\n",
    "    hidden_states_orig = hidden_states\n",
    "    input_ndim = hidden_states.ndim\n",
    "\n",
    "    if input_ndim == 4:\n",
    "        batch_size, channel, height, width = hidden_states.shape\n",
    "        hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n",
    "    \n",
    "    \n",
    "    batch_size, sequence_length, _ = hidden_states.shape\n",
    "    \n",
    "    attention_mask = self.prepare_attention_mask(attention_mask, sequence_length, batch_size=batch_size)\n",
    "    \n",
    "    query = self.to_q(hidden_states)\n",
    "    query = self.head_to_batch_dim(query)\n",
    "    \n",
    "    context_states = text_cond if encoder_hidden_states is not None else hidden_states\n",
    "    key = self.to_k(context_states)\n",
    "    value = self.to_v(context_states)\n",
    "    key = self.head_to_batch_dim(key)\n",
    "    value = self.head_to_batch_dim(value)\n",
    "    \n",
    "    #################################################\n",
    "    \n",
    "    if DENSEDIFF_ON and (COUNT < NUM_INFERENCE_STEPS*reg_part):\n",
    "        # print('DENSE')\n",
    "        dtype = query.dtype\n",
    "        if self.upcast_attention:\n",
    "            query = query.float()\n",
    "            key = key.float()\n",
    "            \n",
    "            \n",
    "        sim = torch.baddbmm(torch.empty(query.shape[0], query.shape[1], key.shape[1], \n",
    "                                        dtype=query.dtype, device=query.device),\n",
    "                            query, key.transpose(-1, -2), beta=0, alpha=self.scale)\n",
    "        \n",
    "        treg = torch.pow(timesteps[COUNT]/1000, 5)\n",
    "\n",
    "        ## reg at self-attn\n",
    "        if encoder_hidden_states is None:\n",
    "            min_value = sim[int(sim.size(0)/2):].min(-1)[0].unsqueeze(-1)\n",
    "            max_value = sim[int(sim.size(0)/2):].max(-1)[0].unsqueeze(-1)  \n",
    "            mask = sreg_maps[sim.size(1)].repeat(self.heads,1,1)\n",
    "            size_reg = reg_sizes[sim.size(1)].repeat(self.heads,1,1)\n",
    "            \n",
    "            sim[int(sim.size(0)/2):] += (mask>0)*size_reg*sreg*treg*(max_value-sim[int(sim.size(0)/2):])\n",
    "            sim[int(sim.size(0)/2):] -= ~(mask>0)*size_reg*sreg*treg*(sim[int(sim.size(0)/2):]-min_value)\n",
    "            \n",
    "        ## reg at cross-attn\n",
    "        else:\n",
    "            min_value = sim[int(sim.size(0)/2):].min(-1)[0].unsqueeze(-1)\n",
    "            max_value = sim[int(sim.size(0)/2):].max(-1)[0].unsqueeze(-1)  \n",
    "            mask = creg_maps[sim.size(1)].repeat(self.heads,1,1)\n",
    "            size_reg = reg_sizes[sim.size(1)].repeat(self.heads,1,1)\n",
    "            \n",
    "            sim[int(sim.size(0)/2):] += (mask>0)*size_reg*creg*treg*(max_value-sim[int(sim.size(0)/2):])\n",
    "            sim[int(sim.size(0)/2):] -= ~(mask>0)*size_reg*creg*treg*(sim[int(sim.size(0)/2):]-min_value)\n",
    "\n",
    "        attention_probs = sim.softmax(dim=-1)\n",
    "        attention_probs = attention_probs.to(dtype)\n",
    "       \n",
    "\n",
    "    else:\n",
    "        # print('No')\n",
    "        attention_probs = self.get_attention_scores(query, key, attention_mask)\n",
    "        \n",
    "    hidden_states = torch.bmm(attention_probs, value)\n",
    "    hidden_states = self.batch_to_head_dim(hidden_states)\n",
    "    \n",
    "    # linear proj\n",
    "    hidden_states = self.to_out[0](hidden_states)\n",
    "    # dropout\n",
    "    hidden_states = self.to_out[1](hidden_states)\n",
    "    # print(residual.shape, hidden_states.shape)\n",
    "    if input_ndim == 4:\n",
    "        hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n",
    "    if self.residual_connection:\n",
    "        hidden_states = hidden_states + residual\n",
    "        \n",
    "\n",
    "    hidden_states = hidden_states / self.rescale_output_factor\n",
    "\n",
    "    return hidden_states\n",
    "\n",
    "\n",
    "def attention2Mod(pipe):\n",
    "    for n, _module in pipe.unet.up_blocks.named_modules():\n",
    "        if _module.__class__.__name__ == \"Attention\":\n",
    "            _module.__class__.__call__ = mod_forward\n",
    "\n",
    "\n",
    "def attention2Orig(pipe, mod_orig):\n",
    "    for n, _module in pipe.unet.up_blocks.named_modules():\n",
    "        if _module.__class__.__name__ == \"Attention\":\n",
    "            _module.__class__.__call__ = mod_orig\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea53155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# views = get_views(global_canvas_H, global_canvas_W, window_size=config.window_size, stride=config.stride, circular_padding=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f835cbb-034a-4f2a-9865-c99add64df8d",
   "metadata": {},
   "source": [
    "### Obtain Pose map, Layout Map, Text Dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a208b8e4-0f36-4096-a80d-d414925e50e9",
   "metadata": {},
   "source": [
    "### Preprocess patch inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e1f1b2-1985-4c2a-8681-240733f580c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# full_prompt_for_views,\\\n",
    "# prompts_for_views,\\\n",
    "# text_cond_for_views,\\\n",
    "# sreg_maps_for_views,\\\n",
    "# reg_sizes_for_views,\\\n",
    "# creg_maps_for_views,\\\n",
    "# prompts_for_view_multi,\\\n",
    "# text_cond_for_view_multi,\\\n",
    "# sreg_maps_for_view_multi,\\\n",
    "# reg_sizes_for_view_multi,\\\n",
    "# creg_maps_for_view_multi,\\\n",
    "# excluded_prompts,\\\n",
    "# pose_image_for_views = preprocess_patch(pipe, config,pose_map,\n",
    "#                     group_L_maps,\n",
    "#                     inst_obj_L_maps,\n",
    "#                     inst_obj_L_maps_small,\n",
    "#                     group_prompt_dic,\n",
    "#                     inst_obj_prompt_dic,\n",
    "#                     global_canvas_H,\n",
    "#                     global_canvas_W,\n",
    "#                     global_prompt,\n",
    "#                     bsz,\n",
    "#                     views)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# full_prompt_for_views: 각 view에 해당하는 full prompt들 -> [view_num, 1] List로 구성\n",
    "\n",
    "# prompt_for_views: 각 view에 해당하는 prompt들 구성 요소 -> [view_num, prompt_component_num+1, 1]\n",
    "\n",
    "# text_cond_for_views: 각 view에 해당하는 prompt들 구성 요소들을 embedding한 결과 -> [view_num, prompt_component_num+1, 1]\n",
    "\n",
    "# views: 64 x 64 기준으로 stride 씩 가면서 뽑은 4 point List \n",
    "\n",
    "# excluded_prompts: [?]\n",
    "\n",
    "# pose_image_for_views: 각 view에 해당하는 human pose(졸라맨들) 들의 PIL.Image List."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e966add5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Code \n",
    "\n",
    "# global_prompt, global_H, global_W, global_canvas_H, global_canvas_W = getGlobalInfo(keylayout_data, config)\n",
    "# group_ids = getGroupIds(group_dictionary, config)\n",
    "\n",
    "\n",
    "# pose_map,\\\n",
    "# group_L_maps,\\\n",
    "# inst_obj_L_maps,\\\n",
    "# inst_obj_boxes, \\\n",
    "# inst_obj_L_maps_small,\\\n",
    "# group_maps,\\\n",
    "# inst_maps,\\\n",
    "# obj_maps,\\\n",
    "# group_prompt_dic,\\\n",
    "# inst_obj_prompt_dic,\\\n",
    "# poses, \\\n",
    "# pose_masks, \\\n",
    "# nose2neck_lengths,\\\n",
    "# group_boxes,\\\n",
    "# inst_boxes,\\\n",
    "# obj_boxes, \\\n",
    "# inst_obj_maps,\\\n",
    "# pose_box_map = generate_map( \n",
    "#               global_canvas_H=global_canvas_H, \n",
    "#               global_canvas_W=global_canvas_W, \n",
    "#               config=config, \n",
    "#               global_prompt=global_prompt,\n",
    "#               group_ids=group_ids,\n",
    "#               group_dictionary=group_dictionary\n",
    "#               )\n",
    "\n",
    "\n",
    "# views = get_views(global_canvas_H, global_canvas_W, window_size=config.window_size, stride=config.stride, circular_padding=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78d264f-8c82-4411-8f19-13a27fad4805",
   "metadata": {},
   "source": [
    "### Generate Images  - MultiDiff+ControlNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72461cee-4cdb-4f2b-8188-5ec047d9e984",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "############################################################################################################\n",
    "\"\"\"\n",
    "view\n",
    "pose_image_for_views\n",
    "\n",
    "full_prompt_for_views\n",
    "\n",
    "text_cond_for_views\n",
    "sreg_maps_for_views\n",
    "reg_sizes_for_views\n",
    "creg_maps_for_views\n",
    "\n",
    "global_prompt\n",
    "global_H\n",
    "global_W\n",
    "global_canvas_H\n",
    "global_canvas_W\n",
    "output_path\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# strength = config.strength\n",
    "# print(f'strength: {strength}')\n",
    "\n",
    "# if config.gen_mode == 'global_focus':\n",
    "#     # mix_mask_for_views = None\n",
    "#     img2img_input_path = None\n",
    "# elif config.gen_mode == 'group_focus':\n",
    "#     # mix_mask_for_views = group_mix_mask_for_views\n",
    "#     # mix_mask = inst_obj_mix_mask#group_mix_mask\n",
    "#     img2img_input_path = config.output_path / f'results_1.png'\n",
    "    \n",
    "# elif config.gen_mode == 'inst_obj_focus':\n",
    "#     # mix_mask_for_views = inst_obj_mix_mask_for_views\n",
    "#     # mix_mask = inst_obj_mix_mask\n",
    "#     img2img_input_path = config.output_path / f'results_2.png'\n",
    "\n",
    "attention2Mod(pipe)\n",
    "\n",
    "for hierar_step in range(3):\n",
    "    if hierar_step == 0:\n",
    "        # initial generation \n",
    "        config.gen_mode = 'global_focus'\n",
    "        config.zoom_ratio = 1\n",
    "        config.use_img2img = False\n",
    "        config.enlarge_ratio = 2\n",
    "        config.strength = 0.\n",
    "        config.mix_ratio = 0.5\n",
    "        config.reverse_ratio = 1.\n",
    "        config.img2img_input_path = None\n",
    "    elif hierar_step == 1:\n",
    "        config.gen_mode = 'group_focus'\n",
    "        config.zoom_ratio = 2\n",
    "        config.use_img2img = True\n",
    "        config.enlarge_ratio = 2\n",
    "        config.strength = 0.7\n",
    "        config.mix_ratio = 0.5\n",
    "        config.reverse_ratio = 0.5\n",
    "        config.img2img_input_path = config.output_path / f'results_global_focus.png' # Later\n",
    "    else:\n",
    "        config.gen_mode = 'inst_obj_focus'\n",
    "        config.zoom_ratio = 4\n",
    "        config.use_img2img = True\n",
    "        config.enlarge_ratio = 2\n",
    "        config.strength = 0.5\n",
    "        config.mix_ratio = 0.5\n",
    "        config.reverse_ratio = 0.5\n",
    "        config.img2img_input_path = config.output_path / f'results_group_focus.png' # Later\n",
    "\n",
    "    print(f\"!!!!!!!!!!!!!!!!!{config.gen_mode, config.zoom_ratio}!!!!!!!!!!!!!!!!!\")\n",
    "\n",
    "    # initialize \n",
    "    reg_part = 0.5\n",
    "    sreg = .3\n",
    "    creg = 1.\n",
    "    COUNT = 0\n",
    "    DENSEDIFF_ON = True\n",
    "    NUM_INFERENCE_STEPS = config.num_inference_steps\n",
    "\n",
    "    global_prompt, global_H, global_W, global_canvas_H, global_canvas_W = getGlobalInfo(keylayout_data, config)\n",
    "    group_ids = getGroupIds(group_dictionary, config)\n",
    "\n",
    "    pose_map,\\\n",
    "    group_L_maps,\\\n",
    "    inst_obj_L_maps,\\\n",
    "    inst_obj_boxes, \\\n",
    "    inst_obj_L_maps_small,\\\n",
    "    group_maps,\\\n",
    "    inst_maps,\\\n",
    "    obj_maps,\\\n",
    "    group_prompt_dic,\\\n",
    "    inst_obj_prompt_dic,\\\n",
    "    poses, \\\n",
    "    pose_masks, \\\n",
    "    nose2neck_lengths,\\\n",
    "    group_boxes,\\\n",
    "    inst_boxes,\\\n",
    "    obj_boxes, \\\n",
    "    inst_obj_maps,\\\n",
    "    pose_box_map, \\\n",
    "    group_mix_mask, \\\n",
    "    inst_obj_mix_mask = generate_map( \n",
    "                global_canvas_H=global_canvas_H, \n",
    "                global_canvas_W=global_canvas_W, \n",
    "                config=config, \n",
    "                global_prompt=global_prompt,\n",
    "                group_ids=group_ids,\n",
    "                group_dictionary=group_dictionary\n",
    "                )\n",
    "\n",
    "\n",
    "    views = get_views(global_canvas_H, global_canvas_W, window_size=config.window_size, stride=config.stride, circular_padding=False)\n",
    "\n",
    "    attention2Mod(pipe)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ### 0. Loading\n",
    "        controlnet = pipe.controlnet._orig_mod if is_compiled_module(pipe.controlnet) else pipe.controlnet\n",
    "        batch_size = 1\n",
    "        device = pipe._execution_device\n",
    "        do_classifier_free_guidance = config.guidance_scale > 1.0 #True\n",
    "        \n",
    "        control_guidance_start, control_guidance_end = [config.control_guidance_start], [config.control_guidance_end]\n",
    "        \n",
    "        ### 1. Encode input prompt\n",
    "        text_encoder_lora_scale = (\n",
    "            config.cross_attention_kwargs.get(\"scale\", None) if config.cross_attention_kwargs is not None else None\n",
    "        )\n",
    "        global_prompt_embeds = pipe._encode_prompt(\n",
    "            global_prompt,\n",
    "            device,\n",
    "            config.num_images_per_prompt,\n",
    "            do_classifier_free_guidance,\n",
    "            negative_prompt=config.negative_prompt,\n",
    "            prompt_embeds=config.prompt_embeds,\n",
    "            negative_prompt_embeds=config.negative_prompt_embeds,\n",
    "            lora_scale=text_encoder_lora_scale,\n",
    "        ) # Not used\n",
    "\n",
    "\n",
    "        # 6. Prepare latent variables\n",
    "\n",
    "        \n",
    "        ### 2. Prepare timesteps ### 3. Prepare latent variables\n",
    "        pipe.scheduler.set_timesteps(config.num_inference_steps, device=device)\n",
    "        num_channels_latents = pipe.unet.config.in_channels\n",
    "\n",
    "        attention2Orig(pipe, mod_forward_orig)\n",
    "\n",
    "        # normal에서는 false\n",
    "        timesteps = pipe.scheduler.timesteps\n",
    "\n",
    "        latents = pipe.prepare_latents(\n",
    "            batch_size,\n",
    "            num_channels_latents,\n",
    "            global_canvas_H,\n",
    "            global_canvas_W,\n",
    "            global_prompt_embeds.dtype,\n",
    "            device,\n",
    "            config.generator,\n",
    "            None,\n",
    "        ) # -> 주석 읽어보기\n",
    "\n",
    "\n",
    "\n",
    "        if config.use_img2img:\n",
    "            COUNT = int(NUM_INFERENCE_STEPS / config.reverse_ratio)\n",
    "            t_mix = int(1000 * (1 - config.reverse_ratio))\n",
    "            mix_steps = int(t_mix/1000 * 50)\n",
    "            timesteps_list = list(range(t_mix+1 - int(1000/NUM_INFERENCE_STEPS), 0, -int(1000/NUM_INFERENCE_STEPS)))\n",
    "            timesteps = torch.tensor(timesteps_list, dtype=torch.int, device='cuda')\n",
    "            low_res_image = Image.open(config.img2img_input_path)\n",
    "            resize_image = double_image(low_res_image, config.enlarge_ratio)\n",
    "\n",
    "            is_odd_h = (global_H % 2)\n",
    "            is_odd_w = (global_W % 2)\n",
    "\n",
    "            img2img_input = torch.ones((1,3,global_canvas_H, global_canvas_W), dtype=torch.half, device=config.device)\n",
    "            img2img_input[:,:,:global_H-is_odd_h,:global_W-is_odd_w] = resize_image\n",
    "            \n",
    "            prior_image_latent = pipe.vae.encode(img2img_input*2 - 1).latent_dist.sample() * 0.18215\n",
    "            latents = pipe.scheduler.add_noise(prior_image_latent, latents, timesteps=torch.tensor([t_mix], dtype=torch.int, device='cuda'))\n",
    "\n",
    "\n",
    "            print(f\"latents_timestep:{t_mix}\")\n",
    "\n",
    "        print(f\"timesteps:{timesteps}\")\n",
    "\n",
    "        \n",
    "        ### 3-1. Prepare black latent variables\n",
    "\n",
    "        pad_image = torch.ones((1, 3, global_canvas_H, global_canvas_W))*255 # global_canvas_H, global_canvas_W 사이즈의 흰색 이미지를 생성 \n",
    "        pad_image = pipe.image_processor.preprocess(pad_image)\n",
    "        pad_image = pad_image.to(next(iter(pipe.vae.post_quant_conv.parameters())).dtype).to(device)\n",
    "        pad_latents = pipe.vae.encode(pad_image).latent_dist.mean\n",
    "        pad_latents = (pipe.vae.config.scaling_factor * pad_latents).to(latents.dtype)\n",
    "\n",
    "        attention2Mod(pipe)\n",
    "\n",
    "        \n",
    "        # 4. Define panorama grid and initialize views for synthesis.\n",
    "        \n",
    "\n",
    "        count = torch.zeros_like(latents)\n",
    "        value = torch.zeros_like(latents)\n",
    "\n",
    "            \n",
    "        # 5. Prepare extra step kwargs. todo: Logic should ideally just be moved out of the pipeline\n",
    "        extra_step_kwargs = pipe.prepare_extra_step_kwargs(config.generator, config.eta)\n",
    "        \n",
    "        # 5.1 Create tensor stating which controlnets to keep\n",
    "        controlnet_keep = []\n",
    "        for i in range(len(timesteps)):\n",
    "            keeps = [\n",
    "                1.0 - float(i / len(timesteps) < s or (i + 1) / len(timesteps) > e)\n",
    "                for s, e in zip(control_guidance_start, control_guidance_end)\n",
    "            ]\n",
    "            controlnet_keep.append(keeps[0] if isinstance(controlnet, ControlNetModel) else keeps)\n",
    "                \n",
    "        \n",
    "        # 6. Denoising loop\n",
    "        num_warmup_steps = len(timesteps) - config.num_inference_steps * pipe.scheduler.order\n",
    "        with pipe.progress_bar(total=len(timesteps)) as progress_bar:\n",
    "            for i, t in enumerate(timesteps):\n",
    "                count.zero_()\n",
    "                value.zero_()\n",
    "\n",
    "                #! When used mix_mask\n",
    "                # if config.use_img2img and i < len(timesteps)-1:\n",
    "                #     # print('hi')\n",
    "                #     # latents = latents_img\n",
    "                    \n",
    "                #     if latent_timesteps[0] == timesteps[i]:\n",
    "                #         print(latent_timesteps[0], timesteps[i])\n",
    "                #         if config.gen_mode == 'group_focus':\n",
    "                #             mix_mask = group_mix_mask.to(latents.dtype).to(latents.device).unsqueeze(0).unsqueeze(0)\n",
    "                #         elif config.gen_mode == 'inst_obj_focus':\n",
    "                #             mix_mask = inst_obj_mix_mask.to(latents.dtype).to(latents.device).unsqueeze(0).unsqueeze(0)\n",
    "                #         latents = mix_mask * latents + (1-mix_mask) * latents_img\n",
    "                \n",
    "                \n",
    "                \n",
    "                # DENSEDIFF_ON = True\n",
    "                # NUM_GROUPS = len(view_batch)\n",
    "                NUM_GROUPS = len(views)\n",
    "\n",
    "                attention2Mod(pipe)\n",
    "\n",
    "                # for j, batch_view in enumerate(view_batch):\n",
    "                for view_index, batch_view in enumerate(views):\n",
    "                    #!!!!! Instance/Object description manipulation\n",
    "                    # inst_obj_prompt_dic[2] = 'Donald Trumph wearing a suit'\n",
    "\n",
    "                    full_prompt_for_view_multi,\\\n",
    "                    prompts_for_view_multi,\\\n",
    "                    text_cond_for_view_multi,\\\n",
    "                    sreg_maps_for_view_multi,\\\n",
    "                    reg_sizes_for_view_multi,\\\n",
    "                    creg_maps_for_view_multi,\\\n",
    "                    excluded_prompts,\\\n",
    "                    pose_image_for_view = preprocess_patch(pipe, config,pose_map,\n",
    "                                        group_L_maps,\n",
    "                                        inst_obj_L_maps,\n",
    "                                        inst_obj_L_maps_small,\n",
    "                                        group_prompt_dic,\n",
    "                                        inst_obj_prompt_dic,\n",
    "                                        global_canvas_H,\n",
    "                                        global_canvas_W,\n",
    "                                        global_prompt,\n",
    "                                        bsz,\n",
    "                                        views, \n",
    "                                        view_index)\n",
    "                    # \n",
    "                    \n",
    "                    # print(f\"i : {i}, view_index:{view_index}\")\n",
    "                    # print(f\"pose_image_for_view :{pose_image_for_view}\")\n",
    "                    # print(f\"sreg_maps_for_view_multi :{sreg_maps_for_view_multi}\")\n",
    "                    # print(f\"reg_sizes_for_view_multi :{reg_sizes_for_view_multi}\")\n",
    "                    # print(f\"creg_maps_for_view_multi :{creg_maps_for_view_multi}\")\n",
    "\n",
    "\n",
    "\n",
    "                    # patch 사이즈로 자른 pose image = image를 prepare_image에 넣어놓음\n",
    "                    control_image_for_views = pipe.prepare_image( ################ H and W\n",
    "                                    image=pose_image_for_view,\n",
    "                                    width=config.window_size*8,\n",
    "                                    height=config.window_size*8,\n",
    "                                    batch_size=batch_size * config.num_images_per_prompt,\n",
    "                                    num_images_per_prompt=config.num_images_per_prompt,\n",
    "                                    device=device,\n",
    "                                    dtype=controlnet.dtype,\n",
    "                                    do_classifier_free_guidance=do_classifier_free_guidance,\n",
    "                                    guess_mode=config.guess_mode,\n",
    "                                ) \n",
    "                    # 각 view의 각 prompt를 따로 encode해서 token으로 바꾼것들의 list를 저장해놓음\n",
    "                    prompt_embeds_for_views_multi = []\n",
    "                    # for prompt in full_prompt_for_views[view_index]:\n",
    "                    for prompt in full_prompt_for_view_multi:\n",
    "                        prompt_embeds_for_views_multi.append(pipe._encode_prompt(\n",
    "                            prompt,\n",
    "                            device,\n",
    "                            config.num_images_per_prompt,\n",
    "                            do_classifier_free_guidance,\n",
    "                            negative_prompt=config.negative_prompt,\n",
    "                            prompt_embeds=config.prompt_embeds,\n",
    "                            negative_prompt_embeds=config.negative_prompt_embeds,\n",
    "                            lora_scale=text_encoder_lora_scale,\n",
    "                        ))                    \n",
    "\n",
    "                \n",
    "\n",
    "                    #! View_batch = 1 always!!!!!\n",
    "                    vb_size = 1\n",
    "                    view_batch = views[view_index]\n",
    "                    views_scheduler_status = [copy.deepcopy(pipe.scheduler.__dict__)] * len(view_batch)\n",
    "                    # control_image_for_views_batch = control_image_for_views[j]\n",
    "                    control_image_for_views_batch = control_image_for_views[0]\n",
    "                    # if config.gen_mode in ['group_focus', 'inst_obj_focus']:\n",
    "                    #     mix_mask_for_views_batch = mix_mask_for_views[view_index]\n",
    "                    \n",
    "                    full_prompt_for_views_batch = full_prompt_for_view_multi\n",
    "                    \n",
    "                    # text_cond_for_views_batch = text_cond_for_view_multi\n",
    "                    # sreg_maps_for_views_batch = sreg_maps_for_view_multi\n",
    "                    # reg_sizes_for_views_batch = reg_sizes_for_view_multi\n",
    "                    # creg_maps_for_views_batch = creg_maps_for_view_multi\n",
    "\n",
    "                    \n",
    "                                \n",
    "                \n",
    "                    # get the latents corresponding to the current view coordinates\n",
    "\n",
    "                    h_start, h_end, w_start, w_end = batch_view\n",
    "                \n",
    "\n",
    "\n",
    "                    latents_for_view = torch.cat(\n",
    "                        [\n",
    "                            latents[:, :, h_start:h_end, w_start:w_end]\n",
    "                        ]\n",
    "                    )\n",
    "\n",
    "                    \n",
    "                    control_image = control_image_for_views_batch.to(latents.dtype).to(latents.device)\n",
    "                    \n",
    "                    text_cond_total = text_cond_for_view_multi\n",
    "                    sreg_maps_total = sreg_maps_for_view_multi\n",
    "                    reg_sizes_total = reg_sizes_for_view_multi\n",
    "                    creg_maps_total = creg_maps_for_view_multi\n",
    "                    # prompt_embeds_total = prompt_embeds_for_views\n",
    "                    prompt_embeds_total = prompt_embeds_for_views_multi\n",
    "                    views_scheduler_status_total = [views_scheduler_status[0]] * len(text_cond_total)\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "                    for k in range(len(text_cond_total)):\n",
    "                        text_cond = text_cond_total[k].to(latents.dtype).to(latents.device)\n",
    "                        sreg_maps = sreg_maps_total[k]\n",
    "                        reg_sizes = reg_sizes_total[k]\n",
    "                        creg_maps = creg_maps_total[k]\n",
    "                        prompt_embeds = prompt_embeds_total[k].to(latents.dtype).to(latents.device)\n",
    "\n",
    "                    \n",
    "                        # rematch block's scheduler status\n",
    "                        pipe.scheduler.__dict__.update(views_scheduler_status_total[k])\n",
    "\n",
    "                        \n",
    "                        # expand the latents if we are doing classifier free guidance\n",
    "                        latent_model_input = (\n",
    "                            latents_for_view.repeat_interleave(2, dim=0)\n",
    "                            if do_classifier_free_guidance\n",
    "                            else latents_for_view\n",
    "                        )\n",
    "                    \n",
    "                        latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)\n",
    "            \n",
    "                        # repeat prompt_embeds for batch\n",
    "                        prompt_embeds_input = torch.cat([prompt_embeds] * vb_size) # TODO: view batch 에 따라 변경해야함\n",
    "        \n",
    "                        \n",
    "                        ############# START: Controlnet, DenseDiff\n",
    "                        # controlnet(s) inference\n",
    "                        # 일단 guess_mode = False상태임, do_classifier_free_guidance 는 True\n",
    "                        if config.guess_mode and do_classifier_free_guidance:\n",
    "                            # Infer ControlNet only for the conditional batch.\n",
    "                            control_model_input = latents_for_view # todo: guess mode 뭐고?\n",
    "                            control_model_input = pipe.scheduler.scale_model_input(control_model_input, t)\n",
    "                            controlnet_prompt_embeds = prompt_embeds_input.chunk(2)[1]\n",
    "                        else:\n",
    "                            control_model_input = latent_model_input\n",
    "                            controlnet_prompt_embeds = prompt_embeds_input\n",
    "            \n",
    "                        if isinstance(controlnet_keep[i], list):\n",
    "                            cond_scale = [c * s for c, s in zip(config.controlnet_conditioning_scale, controlnet_keep[i])]\n",
    "                        else:\n",
    "                            controlnet_cond_scale = config.controlnet_conditioning_scale\n",
    "                            if isinstance(controlnet_cond_scale, list):\n",
    "                                controlnet_cond_scale = controlnet_cond_scale[0]\n",
    "                            cond_scale = controlnet_cond_scale * controlnet_keep[i]\n",
    "                        \n",
    "                        \n",
    "                        down_block_res_samples, mid_block_res_sample = pipe.controlnet(\n",
    "                            control_model_input,\n",
    "                            t,\n",
    "                            encoder_hidden_states=controlnet_prompt_embeds,\n",
    "                            controlnet_cond=control_image,\n",
    "                            conditioning_scale=cond_scale,\n",
    "                            guess_mode=config.guess_mode,\n",
    "                            return_dict=False,\n",
    "                        )\n",
    "            \n",
    "                        if config.guess_mode and do_classifier_free_guidance:\n",
    "                            # Infered ControlNet only for the conditional batch.\n",
    "                            # To apply the output of ControlNet to both the unconditional and conditional batches,\n",
    "                            # add 0 to the unconditional batch to keep it unchanged.\n",
    "                            down_block_res_samples = [torch.cat([torch.zeros_like(d), d]) for d in down_block_res_samples]\n",
    "                            mid_block_res_sample = torch.cat([torch.zeros_like(mid_block_res_sample), mid_block_res_sample])\n",
    "                        # predict the noise residual\n",
    "                        noise_pred = pipe.unet(\n",
    "                            latent_model_input,\n",
    "                            t,\n",
    "                            encoder_hidden_states=prompt_embeds_input,\n",
    "                            cross_attention_kwargs=config.cross_attention_kwargs,\n",
    "                            down_block_additional_residuals=down_block_res_samples,\n",
    "                            mid_block_additional_residual=mid_block_res_sample,\n",
    "                            return_dict=False,\n",
    "                        )[0]\n",
    "            \n",
    "                        # perform guidance\n",
    "                        if do_classifier_free_guidance:\n",
    "                            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2) # noise_pred 반으로 두조각 내기 \n",
    "                            noise_pred = noise_pred_uncond + config.guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "            \n",
    "                        ############# END: Controlnet, DenseDiff\n",
    "                        \n",
    "                    \n",
    "                        # compute the previous noisy sample x_t -> x_t-1\n",
    "                        latents_denoised_batch = pipe.scheduler.step(\n",
    "                            noise_pred, t, latents_for_view, **extra_step_kwargs\n",
    "                        ).prev_sample\n",
    "\n",
    "                        \n",
    "                        # save views scheduler status after sample\n",
    "                        views_scheduler_status[0] = copy.deepcopy(pipe.scheduler.__dict__)\n",
    "\n",
    "                        \n",
    "                        value[:, :, h_start:h_end, w_start:w_end] += latents_denoised_batch\n",
    "                        count[:, :, h_start:h_end, w_start:w_end] += 1\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "                    \n",
    "                # take the MultiDiffusion step. Eq. 5 in MultiDiffusion paper: https://arxiv.org/abs/2302.08113\n",
    "                latents = torch.where(count > 0, value / count, value)\n",
    "                COUNT += 1\n",
    "                \n",
    "\n",
    "                \n",
    "                if use_padded_latents:               \n",
    "                    soft_patting_ratio = 0.5\n",
    "                    border_offset_H =  int( (global_canvas_H - global_H)/8*soft_patting_ratio)\n",
    "                    border_offset_W =  int( (global_canvas_W - global_W)/8*soft_patting_ratio)\n",
    "                    shape = pad_latents.shape\n",
    "                    noise = randn_tensor(shape, generator=config.generator, device=device, dtype=pad_latents.dtype)\n",
    "                    if i != len(timesteps) - 1:\n",
    "                        pad_latents_noisy = pipe.scheduler.add_noise(pad_latents, noise, timesteps[i+1]).to(latents.dtype)\n",
    "                    else:\n",
    "                        pad_latents_noisy = pad_latents\n",
    "                    \n",
    "                    latents[:, :, int(global_H/8)+border_offset_H:, :] = pad_latents_noisy[:, :, int(global_H/8)+border_offset_H:, :]\n",
    "                    latents[:, :, :, int(global_W/8)+border_offset_W:] = pad_latents_noisy[:, :, :, int(global_W/8)+border_offset_W:]\n",
    "                \n",
    "        \n",
    "                # call the callback, if provided\n",
    "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % pipe.scheduler.order == 0):\n",
    "                    progress_bar.update()\n",
    "                    if config.callback is not None and i % config.callback_steps == 0:\n",
    "                        config.callback(i, t, latents)\n",
    "                        \n",
    "\n",
    "            attention2Mod(pipe)\n",
    "            if not config.output_type == \"latent\":\n",
    "                latents = latents.to(next(iter(pipe.vae.post_quant_conv.parameters())).dtype)\n",
    "                image = pipe.vae.decode(latents / pipe.vae.config.scaling_factor, return_dict=False)[0]\n",
    "                image, has_nsfw_concept = pipe.run_safety_checker(image, device, latents.dtype)\n",
    "                has_nsfw_concept = None\n",
    "            else:\n",
    "                image = latents\n",
    "                has_nsfw_concept = None\n",
    "        \n",
    "            if has_nsfw_concept is None:\n",
    "                do_denormalize = [True] * image.shape[0]\n",
    "            else:\n",
    "                do_denormalize = [not has_nsfw for has_nsfw in has_nsfw_concept]\n",
    "            image = image[:,:,:global_H,:global_W]\n",
    "            image = pipe.image_processor.postprocess(image, output_type=config.output_type, do_denormalize=do_denormalize)\n",
    "        \n",
    "    \n",
    "\n",
    "    print(f'* Global prompt: {global_prompt}')\n",
    "    for excluded_prompt in excluded_prompts:\n",
    "        print(f\"[Warning]'{excluded_prompt}' is excluded because it exceeds the token limits.\")\n",
    "    print(f'- image idx: {config.image_idx}')\n",
    "    print(f'- seed: {config.seed}')\n",
    "    print(f'- reg_part: {reg_part}')\n",
    "    print(f'- size: {image[0].size}')\n",
    "\n",
    "    print()\n",
    "    print(f'* Groups')\n",
    "    for i, (group_x1, group_y1, group_x2, group_y2) in enumerate(group_boxes):\n",
    "        key = int(group_maps[i,:,:].max().item())\n",
    "        print(f'green: {group_prompt_dic[key]}')\n",
    "\n",
    "    print()\n",
    "    print(f'* Instance and Objects')\n",
    "    for i, (x1, y1, x2, y2) in enumerate(inst_obj_boxes):\n",
    "        key = int(inst_obj_maps[i,:,:,1].max().item())\n",
    "        print(f'{config.color[i]}: {inst_obj_prompt_dic[key]}')\n",
    "\n",
    "    image[0].save(config.output_path / f'results_{config.gen_mode}.png')\n",
    "\n",
    "    combined_pil = Image.fromarray(np.concatenate([np.asarray(pose_box_map)[:global_H,:global_W,:]]+[np.asarray(image[0])], 1)) # 사이즈 맞춰야함\n",
    "    combined_pil.save(config.output_path / f'results_combined_{config.gen_mode}.png')\n",
    "    combined_pil.save(f\"{config.image_idx}.png\")\n",
    "    display(combined_pil)\n",
    "    # break\n",
    "    \n",
    "    # torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4444dd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_obj_prompt_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d673fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(inst_obj_L_maps.shape)\n",
    "plt.imshow(inst_obj_L_maps[0,...,1].cpu().detach().numpy())\n",
    "plt.colorbar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
